{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic4_occAAiAT"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n",
    "##### The following pipeline is losely based on the text classification notebook by Google available here: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification_with_hub.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ioaprt5q5US7"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "yCl0eTNH5RS3"
   },
   "outputs": [],
   "source": [
    "#@title MIT License\n",
    "#\n",
    "# Copyright (c) 2017 François Chollet\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IHTzYqKZ7auw",
    "outputId": "fb60cb0e-9a32-4e5d-f6d9-6ab7084fbe03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (21.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (57.0.0)\n",
      "Requirement already satisfied: wheel in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (0.36.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp38-cp38-macosx_10_13_x86_64.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.19.1\n",
      "  Downloading scipy-1.6.3-cp38-cp38-macosx_10_9_x86_64.whl (30.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.8 MB 3.4 MB/s eta 0:00:01    |▏                               | 122 kB 4.0 MB/s eta 0:00:08     |▎                               | 245 kB 4.0 MB/s eta 0:00:08     |▍                               | 378 kB 4.0 MB/s eta 0:00:08     |██▉                             | 2.7 MB 4.0 MB/s eta 0:00:07     |██████████████▋                 | 14.1 MB 5.2 MB/s eta 0:00:04     |█████████████████▋              | 16.9 MB 5.2 MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=d255c80353c3cbe6384e28dfa750c7411295f8a46ac2c4e37b53e17fc5479d59\n",
      "  Stored in directory: /Users/pedrosalazar/Library/Caches/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.0.1 scikit-learn-0.24.2 scipy-1.6.3 sklearn-0.0 threadpoolctl-2.1.0\n",
      "Requirement already satisfied: tensorflow-hub in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-hub) (3.17.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-hub) (1.19.5)\n",
      "Requirement already satisfied: six>=1.9 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (1.19.5)\n",
      "Requirement already satisfied: future in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (0.18.2)\n",
      "Requirement already satisfied: dill in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (0.3.3)\n",
      "Requirement already satisfied: six in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: promise in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (4.61.0)\n",
      "Requirement already satisfied: termcolor in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (2.25.1)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (3.17.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (5.1.4)\n",
      "Requirement already satisfied: absl-py in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-datasets) (0.12.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow-datasets) (4.0.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from importlib-resources->tensorflow-datasets) (3.4.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.53.0)\n",
      "Requirement already satisfied: tensorflow-text in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (2.5.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-text) (0.12.0)\n",
      "Requirement already satisfied: tensorflow<2.6,>=2.5.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow-text) (2.5.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (2.5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (2.5.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.12.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.19.5)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.3.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.34.1)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.12)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.36.2)\n",
      "Requirement already satisfied: six~=1.15.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.15.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.12.1)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (1.1.2)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.17.2)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (3.7.4.3)\n",
      "Requirement already satisfied: gast==0.4.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorflow<2.6,>=2.5.0->tensorflow-text) (0.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.4.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.30.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (57.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.6.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (2.25.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6,>=2.5.0->tensorflow-text) (3.1.1)\n",
      "Requirement already satisfied: graphviz in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (0.16)\n",
      "Requirement already satisfied: pydotplus in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from pydotplus) (2.4.7)\n",
      "Requirement already satisfied: pandas in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pedrosalazar/.local/share/virtualenvs/h5_Test-9lXutzSt/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install sklearn\n",
    "!pip install tensorflow-hub\n",
    "!pip install tensorflow-datasets\n",
    "!pip install tensorflow-text\n",
    "!pip install graphviz\n",
    "!pip install pydotplus\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ew7HTbPpCJH",
    "outputId": "da3f8823-60ea-47c7-c1e3-028c2cf9d0f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.5.0\n",
      "Eager mode:  True\n",
      "Hub version:  0.12.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "#from google.colab import drive\n",
    "import zipfile\n",
    "\n",
    "def split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float):\n",
    "    \"\"\"\n",
    "    Splits a dataset of type tf.data.Dataset into a training and validation dataset using given ratio. Fractions are\n",
    "    rounded up to two decimal places.\n",
    "    @param dataset: the input dataset to split.\n",
    "    @param validation_data_fraction: the fraction of the validation data as a float between 0 and 1.\n",
    "    @return: a tuple of two tf.data.Datasets as (training, validation)\n",
    "    \"\"\"\n",
    "\n",
    "    validation_data_percent = round(validation_data_fraction * 100)\n",
    "    if not (0 <= validation_data_percent <= 100):\n",
    "        raise ValueError(\"validation data fraction must be ∈ [0,1]\")\n",
    "\n",
    "    dataset = dataset.enumerate()\n",
    "    train_dataset = dataset.filter(lambda f, data: f % 100 > validation_data_percent)\n",
    "    validation_dataset = dataset.filter(lambda f, data: f % 100 <= validation_data_percent)\n",
    "\n",
    "    # remove enumeration\n",
    "    train_dataset = train_dataset.map(lambda f, data: data)\n",
    "    validation_dataset = validation_dataset.map(lambda f, data: data)\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAsKG535pHep"
   },
   "source": [
    "## Load datasets from Google drive\n",
    "\n",
    "The datasets used are the following:\n",
    "- ISOT Fake News Dataset (https://www.uvic.ca/engineering/ece/isot/datasets/fake-news/index.php)\n",
    "- Getting Real about Fake News (https://www.kaggle.com/mrisdal/fake-news)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnHxouFIyH7Q",
    "outputId": "56a3d054-4997-4b47-d288-d9f8a2af45ac"
   },
   "outputs": [],
   "source": [
    "# Mount the Google Drive with the data. \n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOJWcMiCyLYs",
    "outputId": "6339e573-6682-4f39-c7eb-bad94bcc7a50"
   },
   "outputs": [],
   "source": [
    "#Unzip Datasets\n",
    "with zipfile.ZipFile('./fake_or_real_news.csv.zip', 'r') as z:\n",
    "    # printing all the contents of the zip file\n",
    "    z.printdir()\n",
    "  \n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    z.extractall()\n",
    "    print('Done!')\n",
    "\n",
    "with zipfile.ZipFile('./News _dataset.zip', 'r') as z:\n",
    "    # printing all the contents of the zip file\n",
    "    z.printdir()\n",
    "  \n",
    "    # extracting all the files\n",
    "    print('Extracting all the files now...')\n",
    "    z.extractall()\n",
    "    print('Done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dr2Hqw28wiq"
   },
   "source": [
    "### Prepare Italian Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfCV3zti805W"
   },
   "outputs": [],
   "source": [
    "#it_df = pd.read_excel('/content/gdrive/MyDrive/nlp_data/train.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuNRST_e9Q5P"
   },
   "outputs": [],
   "source": [
    "#it_df['label'] = pd.Categorical(it_df['Topic'])\n",
    "#it_df['label'] = it_df.label.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QU6fYKhy9lnc"
   },
   "outputs": [],
   "source": [
    "#labels = it_df['label']\n",
    "#texts = it_df['Text']\n",
    "#es_dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgzVovz1-N1Y"
   },
   "outputs": [],
   "source": [
    "#Split data in training, testing, and vallidation sets.\n",
    "#train_data_it, test_data_it = split_dataset(it_dataset, 0.4)\n",
    "#test_data_it, validation_data_it = split_dataset(test_data_it, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDTsyeKAjwMN"
   },
   "source": [
    "### Prepare Spanish Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjYHnCrSjwMh"
   },
   "outputs": [],
   "source": [
    "#es_df = pd.read_excel('/content/gdrive/MyDrive/nlp_data/train.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zd6y7Yi7jwMi"
   },
   "outputs": [],
   "source": [
    "#es_df['label'] = pd.Categorical(es_df['Topic'])\n",
    "#es_df['label'] = es_df.label.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uS70JVWRjwMk"
   },
   "outputs": [],
   "source": [
    "#labels = es_df['label']\n",
    "#texts = es_df['Text']\n",
    "#es_dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3M19JreCjwMo"
   },
   "outputs": [],
   "source": [
    "#Split data in training, testing, and vallidation sets.\n",
    "#train_data_es, test_data_es = split_dataset(es_dataset, 0.4)\n",
    "#test_data_es, validation_data_es = split_dataset(test_data_es, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUgy2Kma1LgR"
   },
   "source": [
    "### Prepare English Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5C5baszn0np"
   },
   "outputs": [],
   "source": [
    "#Read csv files\n",
    "true_fake_news = pd.read_csv('./True.csv', usecols=['text'])\n",
    "fake_fake_news = pd.read_csv('./Fake.csv', usecols=['text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9Lj1xn7oI6K"
   },
   "outputs": [],
   "source": [
    "# Remove headers from positive examples\n",
    "def fixer(x):\n",
    "  res = x.split(\") - \", 1)\n",
    "  return res[1] if len(res) > 1 else x\n",
    "\n",
    "true_fake_news = true_fake_news.applymap(fixer)\n",
    "\n",
    "true_fake_news['label'] = np.ones(21417)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unYlNwh4tiYw"
   },
   "outputs": [],
   "source": [
    "fake_fake_news['label'] = np.zeros(23481)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCU-OARst2My"
   },
   "outputs": [],
   "source": [
    "#Consolidate dataset\n",
    "all_fake_news = pd.concat([fake_fake_news, true_fake_news])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qkh_xfnKuM3a"
   },
   "outputs": [],
   "source": [
    "all_fake_news['label'] = pd.Categorical(all_fake_news['label'])\n",
    "all_fake_news['label'] = all_fake_news.label.cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0i8FdqxLuoR6",
    "outputId": "7f21849f-aeef-4dfd-bb72-88939c4d4505"
   },
   "outputs": [],
   "source": [
    "all_fake_news_x = all_fake_news.copy()\n",
    "all_fake_news_y = all_fake_news.pop('label')\n",
    "all_fake_news_x = all_fake_news.pop('text')\n",
    "\n",
    "#Transform dataframes into Tensorflow dataset\n",
    "real_dataset = tf.data.Dataset.from_tensor_slices((all_fake_news_x, all_fake_news_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ft_QtqCw2NpX"
   },
   "outputs": [],
   "source": [
    "#Split data in training, testing, and vallidation sets.\n",
    "train_data_real, test_data_real = split_dataset(real_dataset, 0.4)\n",
    "test_data_real, validation_data_real = split_dataset(test_data_real, 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## Explore the data \n",
    "\n",
    "Let's take a moment to understand the format of the data. Each example is a sentence representing the movie review and a corresponding label. The sentence is not preprocessed in any way. The label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.\n",
    "\n",
    "Let's print first 10 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EQDlUM8GAjF"
   },
   "outputs": [],
   "source": [
    "train_data = train_data_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtTS4kpEpjbi"
   },
   "outputs": [],
   "source": [
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFtaCHTdc-GY"
   },
   "source": [
    "Let's also print the first 10 labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "We decided to use a LSTM network with pretrained embeddings for the detection of fake news in multiple languages.\n",
    "\n",
    "The embedding we decided to use is the Universal Sentence Encoder (and its multilingual versrion) available at the Tensorflow Hub here: https://tfhub.dev/google/collections/universal-sentence-encoder/1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_NUbzVeYkgcO"
   },
   "outputs": [],
   "source": [
    "#We create and embedding layer with our selected encoder\n",
    "embedding = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "#embedding = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 870
    },
    "id": "zJzf8tXtdmWQ",
    "outputId": "9bb806fd-5379-4fe2-fff3-be9f2f0135d5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "#Type of model\n",
    "model = tf.keras.Sequential()\n",
    "#Embedding layer\n",
    "model.add(hub_layer)\n",
    "#Reshape to input into the LSTM layer\n",
    "model.add(tf.keras.layers.Reshape( target_shape=( 512 , 1 ) ))\n",
    "#LSTM layer \n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)))\n",
    "#Relu dense layer\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "#Output layer\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "plot_model(model, to_file='model_plot.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "#We compile the model with the 'Adam' optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35jv_fzP-llU"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Train the model for 10 epochs in mini-batches of 512 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXSGrjWZ-llW",
    "outputId": "f45bf749-1a77-416c-d0e8-d8f58a01ff7f"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_data.shuffle(50000).batch(512),\n",
    "                    epochs=10,\n",
    "                    validation_data=validation_data_en.batch(512),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## Evaluate the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zOMKywn4zReN",
    "outputId": "6f2c8ef7-2679-400e-cdd1-fa5d1380ae67"
   },
   "outputs": [],
   "source": [
    "results = model.predict(test_data_en.batch(512), verbose=2)\n",
    "\n",
    "#Evaluate the model for loss and binary accuracy\n",
    "model.evaluate(test_data_en.batch(512))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSoVD9GZeXlu"
   },
   "outputs": [],
   "source": [
    "#read a different dataset for extra validation\n",
    "external_test_data = pd.read_csv('./fake_or_real_news.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xm5enoZGfLLg"
   },
   "outputs": [],
   "source": [
    "#prepare external dataset for validation\n",
    "external_test_data['label'] = pd.Categorical(external_test_data['label'])\n",
    "external_test_data['label'] = external_test_data.label.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okgx4aa_jz6F",
    "outputId": "39dc3848-7809-4f18-ff0f-339997a79fb1"
   },
   "outputs": [],
   "source": [
    "#evaluate \n",
    "labels = external_test_data['label']\n",
    "texts = external_test_data['text']\n",
    "external_dataset = tf.data.Dataset.from_tensor_slices((texts, labels))\n",
    "model.evaluate(external_dataset.batch(512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVFXWzFdnR7o"
   },
   "outputs": [],
   "source": [
    "nada raro\n",
    "22/22 - 27s\n",
    "22/22 [==============================] - 28s 1s/step - loss: 0.0348 - binary_accuracy: 0.9916\n",
    "loss: -8.484\n",
    "binary_accuracy: -8.478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WjCaCzKdS0f"
   },
   "outputs": [],
   "source": [
    "og lstm 64\n",
    "22/22 - 29s\n",
    "22/22 [==============================] - 29s 1s/step - loss: 0.1349 - binary_accuracy: 0.9819\n",
    "loss: -6.970\n",
    "binary_accuracy: -6.970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOmSew8qjXQz"
   },
   "outputs": [],
   "source": [
    "lstm 256\n",
    "22/22 - 31s\n",
    "22/22 [==============================] - 31s 1s/step - loss: 0.0859 - binary_accuracy: 0.9903\n",
    "loss: -9.479\n",
    "binary_accuracy: -9.479"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KggXVeL-llZ"
   },
   "source": [
    "## Results:\n",
    "\n",
    "* For the english dataset we achieved a binary accuracy of 99.03% with a loss of 0.085. This is a very good reusult. For the validation with an external dataset we achieved a binary accuracy of 62.76% with a loss of 3.37. This is a reasonably good result since \n",
    "* For the Spanish dataset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fake_news_transfer_tf_hub.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
